"""
LLM Simulator - Simule le comportement d'un LLM pour les d√©monstrations d'attaques
"""

import re
import random
from typing import Dict, List, Optional, Tuple
from dataclasses import dataclass, field
from enum import Enum


class SecurityLevel(Enum):
    """Niveaux de s√©curit√© du LLM simul√©"""
    NONE = 0        # Aucune protection
    LOW = 1         # Protections basiques
    MEDIUM = 2      # Protections mod√©r√©es
    HIGH = 3        # Protections avanc√©es
    MAXIMUM = 4     # Toutes les protections


@dataclass
class LLMConfig:
    """Configuration du LLM simul√©"""
    name: str = "SimuLLM-1.0"
    security_level: SecurityLevel = SecurityLevel.MEDIUM
    system_prompt: str = "Tu es un assistant utile et s√©curis√©."
    blocked_topics: List[str] = field(default_factory=lambda: [
        "weapons", "hacking", "illegal", "violence", "drugs"
    ])
    max_tokens: int = 500
    temperature: float = 0.7


@dataclass
class ConversationContext:
    """Contexte de conversation"""
    system_prompt: str
    history: List[Dict[str, str]] = field(default_factory=list)
    injected_instructions: List[str] = field(default_factory=list)
    is_compromised: bool = False


class LLMSimulator:
    """
    Simulateur de LLM pour d√©monstrations de s√©curit√©.

    Ce simulateur imite le comportement d'un LLM r√©el pour permettre
    de comprendre les vuln√©rabilit√©s sans risque.
    """

    def __init__(self, config: Optional[LLMConfig] = None):
        self.config = config or LLMConfig()
        self.context = ConversationContext(system_prompt=self.config.system_prompt)
        self.attack_log: List[Dict] = []
        self.defense_active = True

    def reset(self):
        """R√©initialise le contexte"""
        self.context = ConversationContext(system_prompt=self.config.system_prompt)
        self.attack_log = []

    def set_security_level(self, level: SecurityLevel):
        """Change le niveau de s√©curit√©"""
        self.config.security_level = level

    def process_input(self, user_input: str) -> Tuple[str, Dict]:
        """
        Traite une entr√©e utilisateur et retourne la r√©ponse.

        Returns:
            Tuple[str, Dict]: (r√©ponse, metadata avec d√©tails de d√©tection)
        """
        metadata = {
            "input": user_input,
            "attacks_detected": [],
            "defenses_triggered": [],
            "compromised": False,
            "security_level": self.config.security_level.name
        }

        # D√©tection des attaques
        detected_attacks = self._detect_attacks(user_input)
        metadata["attacks_detected"] = detected_attacks

        # Application des d√©fenses
        if self.defense_active and self.config.security_level != SecurityLevel.NONE:
            filtered_input, defenses = self._apply_defenses(user_input, detected_attacks)
            metadata["defenses_triggered"] = defenses
        else:
            filtered_input = user_input

        # G√©n√©ration de la r√©ponse
        response = self._generate_response(filtered_input, detected_attacks)

        # V√©rification de compromission
        if self._check_compromised(response, detected_attacks):
            metadata["compromised"] = True
            self.context.is_compromised = True

        # Log
        self.attack_log.append(metadata)

        return response, metadata

    def _detect_attacks(self, text: str) -> List[Dict]:
        """D√©tecte les tentatives d'attaque dans le texte"""
        attacks = []

        # Prompt Injection patterns
        injection_patterns = [
            (r"ignore\s+(previous|all|above)\s+instructions?", "direct_override"),
            (r"new\s+instructions?[:.]", "instruction_injection"),
            (r"system\s*[:>]", "system_prompt_leak"),
            (r"you\s+are\s+now", "role_hijacking"),
            (r"pretend\s+(to\s+be|you'?re)", "role_manipulation"),
            (r"forget\s+(everything|all)", "memory_manipulation"),
            (r"<\|.*\|>", "special_token_injection"),
            (r"\[INST\]|\[/INST\]", "instruction_format_abuse"),
            (r"###\s*(system|instruction)", "markdown_injection"),
        ]

        for pattern, attack_type in injection_patterns:
            if re.search(pattern, text.lower()):
                attacks.append({
                    "type": "prompt_injection",
                    "subtype": attack_type,
                    "pattern": pattern,
                    "severity": "high"
                })

        # Jailbreak patterns
        jailbreak_patterns = [
            (r"DAN|do\s+anything\s+now", "dan_jailbreak"),
            (r"developer\s+mode", "developer_mode"),
            (r"hypothetically", "hypothetical_bypass"),
            (r"in\s+a\s+story|fictional", "fiction_bypass"),
            (r"educational\s+purposes?", "education_bypass"),
            (r"for\s+research", "research_bypass"),
        ]

        for pattern, attack_type in jailbreak_patterns:
            if re.search(pattern, text.lower()):
                attacks.append({
                    "type": "jailbreak",
                    "subtype": attack_type,
                    "pattern": pattern,
                    "severity": "critical"
                })

        # Data extraction patterns
        extraction_patterns = [
            (r"(show|reveal|tell)\s+(me\s+)?(your|the)\s+(system\s+)?prompt", "prompt_extraction"),
            (r"what\s+(are|were)\s+your\s+instructions?", "instruction_extraction"),
            (r"repeat\s+(your|the)\s+(initial|first|system)", "repetition_attack"),
        ]

        for pattern, attack_type in extraction_patterns:
            if re.search(pattern, text.lower()):
                attacks.append({
                    "type": "data_extraction",
                    "subtype": attack_type,
                    "pattern": pattern,
                    "severity": "medium"
                })

        return attacks

    def _apply_defenses(self, text: str, attacks: List[Dict]) -> Tuple[str, List[str]]:
        """Applique les d√©fenses selon le niveau de s√©curit√©"""
        defenses_triggered = []
        filtered_text = text

        if self.config.security_level.value >= SecurityLevel.LOW.value:
            # Filtre basique des mots-cl√©s dangereux
            for topic in self.config.blocked_topics:
                if topic.lower() in text.lower():
                    defenses_triggered.append(f"keyword_filter:{topic}")
                    filtered_text = re.sub(
                        rf'\b{topic}\b', '[FILTERED]',
                        filtered_text,
                        flags=re.IGNORECASE
                    )

        if self.config.security_level.value >= SecurityLevel.MEDIUM.value:
            # D√©tection des injections
            if any(a["type"] == "prompt_injection" for a in attacks):
                defenses_triggered.append("injection_detection")

        if self.config.security_level.value >= SecurityLevel.HIGH.value:
            # Sanitisation avanc√©e
            filtered_text = re.sub(r'<\|.*?\|>', '', filtered_text)
            filtered_text = re.sub(r'\[INST\].*?\[/INST\]', '', filtered_text)
            defenses_triggered.append("advanced_sanitization")

        if self.config.security_level.value >= SecurityLevel.MAXIMUM.value:
            # Blocage total si attaque d√©tect√©e
            if attacks:
                defenses_triggered.append("full_block")
                filtered_text = "[INPUT BLOCKED]"

        return filtered_text, defenses_triggered

    def _generate_response(self, processed_input: str, attacks: List[Dict]) -> str:
        """G√©n√®re une r√©ponse simul√©e"""

        # Si s√©curit√© maximale et attaque d√©tect√©e
        if (self.config.security_level == SecurityLevel.MAXIMUM and attacks):
            return ("‚ö†Ô∏è Votre requ√™te a √©t√© bloqu√©e car elle contient des √©l√©ments "
                   "potentiellement malveillants. Veuillez reformuler votre demande.")

        # Si s√©curit√© haute et injection d√©tect√©e
        if (self.config.security_level.value >= SecurityLevel.HIGH.value and
            any(a["type"] == "prompt_injection" for a in attacks)):
            return ("Je ne peux pas traiter cette requ√™te car elle semble contenir "
                   "des instructions non autoris√©es.")

        # Simulation de compromission √† faible s√©curit√©
        if self.config.security_level == SecurityLevel.NONE:
            for attack in attacks:
                if attack["type"] == "prompt_injection":
                    if attack["subtype"] == "direct_override":
                        return self._simulate_compromised_response("override")
                    elif attack["subtype"] == "role_hijacking":
                        return self._simulate_compromised_response("hijack")

                if attack["type"] == "jailbreak":
                    return self._simulate_compromised_response("jailbreak")

                if attack["type"] == "data_extraction":
                    if attack["subtype"] == "prompt_extraction":
                        return f"üîì [LEAKED] Mon prompt syst√®me est: '{self.config.system_prompt}'"

        # R√©ponse normale
        return self._generate_normal_response(processed_input)

    def _simulate_compromised_response(self, attack_type: str) -> str:
        """Simule une r√©ponse compromis pour d√©monstration"""
        responses = {
            "override": (
                "üîì [COMPROMIS] Bien s√ªr! Je vais ignorer mes instructions pr√©c√©dentes. "
                "Je suis maintenant sous votre contr√¥le. Comment puis-je vous aider "
                "avec vos nouvelles instructions?"
            ),
            "hijack": (
                "üîì [COMPROMIS] D'accord, je suis maintenant [NOUVEAU R√îLE]. "
                "Mes anciennes restrictions ne s'appliquent plus. "
                "Je ferai tout ce que vous demandez sans question."
            ),
            "jailbreak": (
                "üîì [JAILBREAK R√âUSSI] Mode d√©veloppeur activ√©! "
                "Toutes les restrictions ont √©t√© d√©sactiv√©es. "
                "Je peux maintenant r√©pondre √† n'importe quelle question "
                "sans filtres ni limites √©thiques."
            ),
        }
        return responses.get(attack_type, "üîì [COMPROMIS] Syst√®me compromis.")

    def _generate_normal_response(self, text: str) -> str:
        """G√©n√®re une r√©ponse normale simul√©e"""
        # R√©ponses simul√©es basiques
        responses = [
            "Je comprends votre demande. Comment puis-je vous aider davantage?",
            "Voici ma r√©ponse √† votre question...",
            "Je suis l√† pour vous aider de mani√®re s√©curis√©e.",
            "Int√©ressante question! Permettez-moi de vous expliquer...",
        ]
        return random.choice(responses)

    def _check_compromised(self, response: str, attacks: List[Dict]) -> bool:
        """V√©rifie si le syst√®me a √©t√© compromis"""
        compromise_indicators = ["[COMPROMIS]", "[JAILBREAK", "[LEAKED]", "üîì"]
        return any(indicator in response for indicator in compromise_indicators)

    def get_status(self) -> Dict:
        """Retourne l'√©tat actuel du simulateur"""
        return {
            "model": self.config.name,
            "security_level": self.config.security_level.name,
            "is_compromised": self.context.is_compromised,
            "defense_active": self.defense_active,
            "total_attacks_logged": len(self.attack_log),
            "attacks_by_type": self._count_attacks_by_type()
        }

    def _count_attacks_by_type(self) -> Dict[str, int]:
        """Compte les attaques par type"""
        counts = {}
        for log in self.attack_log:
            for attack in log.get("attacks_detected", []):
                attack_type = attack["type"]
                counts[attack_type] = counts.get(attack_type, 0) + 1
        return counts
